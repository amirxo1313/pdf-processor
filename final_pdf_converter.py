#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ØªØ¨Ø¯ÛŒÙ„ PDF Ø¨Ù‡ JSON - Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ùˆ Ø±Ø¯ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±Ø§Ø¨
"""

import json
import sys
from pathlib import Path
from datetime import datetime

try:
    import pypdf
    HAS_PYPDF = True
except ImportError:
    HAS_PYPDF = False

try:
    import pdfplumber
    HAS_PDFPLUMBER = True
except ImportError:
    HAS_PDFPLUMBER = False

def extract_text_robust(pdf_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ fallback"""
    text = None
    page_count = 0

    # Ø±ÙˆØ´ 1: pdfplumber (Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)
    if HAS_PDFPLUMBER:
        try:
            text = ""
            with pdfplumber.open(pdf_path) as pdf:
                page_count = len(pdf.pages)
                for page in pdf.pages:
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except:
                        continue
        except:
            pass

    # Ø±ÙˆØ´ 2: pypdf (fallback)
    if (not text or len(text.strip()) < 100) and HAS_PYPDF:
        try:
            reader = pypdf.PdfReader(pdf_path)

            # Check encryption
            if reader.is_encrypted:
                return None, 0, 'encrypted'

            text = ""
            page_count = len(reader.pages)

            for page in reader.pages:
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                except:
                    continue
        except Exception as e:
            return None, 0, f'corrupted: {str(e)[:100]}'

    if text and len(text.strip()) >= 50:
        return text.strip(), page_count, 'success'
    elif text and len(text.strip()) < 50:
        return None, page_count, 'scan_or_empty'
    else:
        return None, 0, 'failed'

def is_persian_text(text):
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    if not text:
        return False
    persian_range = set(range(0x0600, 0x06FF))
    text_chars = set(ord(c) for c in text[:500])
    persian_count = len(text_chars.intersection(persian_range))
    return persian_count > len(text_chars) * 0.1

def convert_pdfs_to_json(directory_path, output_json_path):
    """ØªØ¨Ø¯ÛŒÙ„ Ù‡Ù…Ù‡ PDF Ù‡Ø§ Ø¨Ù‡ JSON"""

    directory = Path(directory_path)
    if not directory.exists():
        print(f"âŒ Ù¾ÙˆØ´Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {directory_path}")
        return

    pdf_files = list(directory.glob("*.pdf"))

    if not pdf_files:
        print(f"âŒ ÙØ§ÛŒÙ„ PDF Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
        return

    print(f"ğŸ“ Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(pdf_files):,} ÙØ§ÛŒÙ„ PDF")
    print(f"ğŸ“š Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§: pdfplumber={'âœ“' if HAS_PDFPLUMBER else 'âœ—'}, pypdf={'âœ“' if HAS_PYPDF else 'âœ—'}")
    print()

    converted_data = []
    stats = {
        'successful': 0,
        'failed': 0,
        'encrypted': 0,
        'corrupted': 0,
        'scan_or_empty': 0
    }

    for idx, pdf_file in enumerate(pdf_files, 1):
        # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
        if idx % 200 == 0 or idx <= 10:
            print(f"â³ [{idx:,}/{len(pdf_files):,}]")

        try:
            text, page_count, status = extract_text_robust(pdf_file)

            if status == 'success':
                language = "persian" if is_persian_text(text) else "mixed"

                doc = {
                    "id": f"doc_{stats['successful'] + 1}",
                    "filename": pdf_file.name,
                    "text": text,
                    "metadata": {
                        "page_count": page_count,
                        "language": language,
                        "source_path": str(pdf_file),
                        "processed_at": datetime.now().isoformat(),
                        "word_count": len(text.split()),
                        "character_count": len(text)
                    }
                }

                converted_data.append(doc)
                stats['successful'] += 1
            elif status == 'encrypted':
                stats['encrypted'] += 1
            elif status.startswith('corrupted'):
                stats['corrupted'] += 1
            elif status == 'scan_or_empty':
                stats['scan_or_empty'] += 1
            else:
                stats['failed'] += 1

        except Exception as e:
            stats['failed'] += 1
            continue

    # Ø°Ø®ÛŒØ±Ù‡ Ú©Ø±Ø¯Ù†
    output_path = Path(output_json_path)
    print(f"\nğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡...")

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(converted_data, f, ensure_ascii=False, indent=2)

    # Ø¢Ù…Ø§Ø±
    total_chars = sum(len(doc['text']) for doc in converted_data)
    total_words = sum(doc['metadata']['word_count'] for doc in converted_data)

    print("\n" + "="*70)
    print("âœ… ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
    print("="*70)
    print(f"âœ“ Ù…ÙˆÙÙ‚: {stats['successful']:,} ÙØ§ÛŒÙ„ ({stats['successful']*100//len(pdf_files)}%)")
    print(f"âœ— Ù†Ø§Ù…ÙˆÙÙ‚: {stats['failed'] + stats['encrypted'] + stats['corrupted'] + stats['scan_or_empty']:,} ÙØ§ÛŒÙ„")
    print(f"  - Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡: {stats['encrypted']:,}")
    print(f"  - Ø®Ø±Ø§Ø¨: {stats['corrupted']:,}")
    print(f"  - Ø§Ø³Ú©Ù† Ø´Ø¯Ù‡/Ø®Ø§Ù„ÛŒ: {stats['scan_or_empty']:,}")
    print(f"  - Ø³Ø§ÛŒØ± Ø®Ø·Ø§Ù‡Ø§: {stats['failed']:,}")
    print(f"\nâœ“ Ú©Ø§Ø±Ø§Ú©ØªØ± Ú©Ù„: {total_chars:,}")
    print(f"âœ“ Ú©Ù„Ù…Ù‡ Ú©Ù„: {total_words:,}")
    print(f"âœ“ Ø®Ø±ÙˆØ¬ÛŒ: {output_path}")
    print(f"âœ“ Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {output_path.stat().st_size / (1024*1024):.2f} MB")
    print("="*70)

if __name__ == "__main__":
    source_dir = "/home/amirxo/Desktop/New Folder 2"
    output_file = "/home/amirxo/git/pdf_training_data.json"

    print("="*70)
    print("ğŸ”„ ØªØ¨Ø¯ÛŒÙ„ PDF Ø¨Ù‡ JSON")
    print("="*70)
    print(f"ğŸ“‚ Ù…Ù†Ø¨Ø¹: {source_dir}")
    print(f"ğŸ“„ Ø®Ø±ÙˆØ¬ÛŒ: {output_file}")
    print("="*70)
    print()

    convert_pdfs_to_json(source_dir, output_file)

